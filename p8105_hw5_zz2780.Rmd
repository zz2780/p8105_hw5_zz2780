---
title: "p8105_hw5_zz2780"
author: "Lydia Zhang"
date: "2022-11-15"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

Problem 1
Import the data
```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```
Tidy the data
```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```
Graph

"This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way."
```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```
Problem 2

Load the data from Github Repo
```{r}
data2<-read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

The raw data contains each homicide's case number, reported date, victims' name, race, age, gender, the city and state of homicide, its specific longitude and latitude, and the disposition.

The code below created variables city_state, grouped by cities and summarized the number of total or unsolved homicides within each city.
```{r}
data2_summary<-data2%>%
  mutate(
    city_state=paste(city, state, sep=", "))%>%
  group_by(city_state)%>%
 summarize(total=n(),
           unsolved=sum(disposition!="Closed by arrest"))

data2_summary
```
I subset the data summary to only city of Baltimore, ran the proportional test, and stored the prop test result as dataframe bal.test.
```{r}
baltimore<-data2_summary%>%
  subset(city_state=="Baltimore, MD")
bal.test<-broom::tidy(prop.test(baltimore$unsolved, baltimore$total))
```

I ran prop.test on every city, stored the test results in new variable test, and extract estimates and 95%CI from the test variable. Finally I deleted test variable for tidy purpose.
```{r}
data2_prop_test<-data2_summary%>%
  mutate(
    test=purrr::map2(unsolved, total, prop.test),
    estimate=map_dbl(test,~.x[["estimate"]]),
    ci_lower=map_dbl(test,~.x[["conf.int"]][1]),
    ci_upper=map_dbl(test,~.x[["conf.int"]][2])
    )%>%
  select(-test)

data2_prop_test
```
The bar plot below shows, in descending order, the proportion of unsolved homicides in each city as well as each 95% CI.
```{r}
ggplot(data2_prop_test, aes(x =reorder(city_state, desc(estimate)), y = estimate)) +
  geom_bar(stat="identity")+
  geom_errorbar(width=.1, aes(ymin=ci_lower,ymax=ci_upper),position=position_dodge(width=0.5))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
  labs(x="City, State", y = "Proportion of Unsolved Homicide",title="Proportion of Unsolved Homicide in Each City")
```

Problem 3

I used for loop to produce 5000 datasets with fixed n, mean, and sd.
```{r}
output=vector("list", 5000)

for(i in 1:5000){
  output[[i]]=rnorm(n=30, mean=0, sd=5)
}
```
I created a list called test to record th t.test results of each dataset, combined the list of test as dataframe test_results, and only selected estimate and p-value.
```{r}
test=vector("list", 5000)

for(i in 1:5000){
  test[[i]]=broom::tidy(t.test(output[[i]]))
}

test_results_0=bind_rows(test)%>%
  select(estimate, p.value)%>%
  mutate(
    true_mean=0
  )
```

I wrote a for loop outside the simulation of 5000 datasets for each true mean (1 to 6), and the final dataframe containing the estimates, p-value, and the true mean of each dataset. 
```{r}
test_results_total=data.frame()
for (a in 1:6){
  output=vector("list", 5000)
  test=vector("list", 5000)
for(i in 1:5000){
  output[[i]]=rnorm(n=30, mean=a, sd=5)
}
for(i in 1:5000){
  test[[i]]=broom::tidy(t.test(output[[i]]))
}
test_results=bind_rows(test)%>%
  select(estimate, p.value)%>%
  mutate(
    true_mean=a
  )
test_results_total=rbind(test_results_total,test_results)
}

test_results_total
```

I combined the test results for mean=0 with mean of 1 to 6, created a new variable rejected to distinguish those the null was rejected (p-value<0.05) from those the null was not rejected (p-value>=0.05). I grouped the dataset by true mean and rejected, calculated the frequency of each proportion. Because we only want to see the proportion of times the null was rejected, I filtered out rejected==No, and produced a line graph to show the association between the proportion of times the null was rejected and the true value of mu. 

I observed that the larger the effect size, the higher the power is. 
```{r}
test_results_0_to_6<-rbind(test_results_0, test_results_total)
test_results_0_to_6%>%
  mutate(
    rejected=case_when(p.value<0.05 ~ "Yes",
                       TRUE~"No")
  )%>%
  group_by(true_mean, rejected)%>%
  summarize(n=n())%>%
  mutate(freq=n/sum(n))%>%
  filter(rejected=="Yes")%>%
  ggplot(aes(x=true_mean, y=freq))+
  geom_line()+
  labs(x="True Mean", y="Power")
```

I first created a sub-dataset with only those the null is rejected, and calculated their estimated mean grouping by true mean. Then I modified the original dataset, calculated the total average of estimated mean for each group of true mean, produced a line plot, then added the rejected sample estimate mean as the red line in the graph.

I can observe that the sample average of estimated mean across tests for which the null is rejected are NOT approximately equal to the true value of mean when the true mean has a value of 1, 2, and 3. And their values converge as the true mean value grows larger. 
```{r}
test_results_0_to_6_rejected<-test_results_0_to_6%>%
  mutate(
    rejected=case_when(p.value<0.05 ~ "Yes",
                       TRUE~"No")
  )%>%
  filter(rejected=="Yes")%>%
  group_by(true_mean)%>%
  summarize(avg_total=mean(estimate))
  
test_results_0_to_6%>%
  group_by(true_mean)%>%
  summarize(avg_total=mean(estimate))%>%
  ggplot(aes(x=true_mean, y=avg_total))+
  geom_line()+
  geom_line(data=test_results_0_to_6_rejected, color="red")+
  labs(x="True mean", y="Average estimated mean")
```





